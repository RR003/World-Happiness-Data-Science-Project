# -*- coding: utf-8 -*-
"""CountriesHappinessAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aNjvtmVvV14Kx-ga38MSxFOiUHayg-VG

# **CSE351 FINAL PROJECT**

## What makes people in a country happy?
Iman Ali, Rahul Raja & Acero
"""

# Import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr

"""## **Load the data**
Data Source: https://worldhappiness.report
"""

## google colab command to import data
from google.colab import files
data1 = files.upload()
data2 = files.upload()
data3 = files.upload()
data4 = files.upload()
data5 = files.upload()

# Use pandas to load data frames

happiness15 = pd.read_csv("2015.csv")
happiness16 = pd.read_csv("2016.csv")
happiness17 = pd.read_csv("2017.csv")
happiness18 = pd.read_csv("2018.csv")
happiness19 = pd.read_csv("2019.csv")

## happiness15 = pd.read_csv("world_happiness_data/2015.csv")
## happiness16 = pd.read_csv("world_happiness_data/2016.csv")
## happiness17 = pd.read_csv("world_happiness_data/2017.csv")
## happiness18 = pd.read_csv("world_happiness_data/2018.csv")
## happiness19 = pd.read_csv("world_happiness_data/2019.csv")

"""# **EDA**
### Get familiar with the dataset and decide what features and observations will be useful 
### (by Iman Ali)
"""

# Display first 5 rows of the 2015 data set
happiness15.head()

# Display first 5 rows of the 2019 data set
happiness19.head()

# Above exploration shows that different years have different attributes
# Data columns are inconsistent for each year

# Check name and type of columns in every years data
happiness15.dtypes

happiness16.dtypes

happiness17.dtypes

happiness18.dtypes

happiness19.dtypes

"""## **Clean and Merge the data (Task 1)**
### By Iman Ali
"""

# Format all data to be uniform and consistent with each other
# by renaming columns to :  
# Country - Rank - Score - GDP per Capita - Life Expectancy - Family/Social Support - Freedom - Corruption - Generosity 

# This step will eliminate conflicts when merging the data sets
# and filter only the useful features/observations of the data for our analysis.

# 2015 
happiness15.rename(columns={
    "Country": "Country",
    "Happiness Rank": "Rank",
    "Happiness Score": "Score",
    "Economy (GDP per Capita)": "GDP per Capita",
    "Health (Life Expectancy)": "Life Expectancy",
    "Family": "Family/Social Support",
    "Freedom": "Freedom",
    "Trust (Government Corruption)": "Corruption",
    "Generosity": "Generosity"
},inplace=True)
happiness15.drop(['Region', 'Standard Error', 'Dystopia Residual'], axis=1, inplace=True)

# 2016
happiness16.rename(columns={
    "Country": "Country",
    "Happiness Rank": "Rank",
    "Happiness Score": "Score",
    "Economy (GDP per Capita)": "GDP per Capita",
    "Health (Life Expectancy)": "Life Expectancy",
    "Family": "Family/Social Support",
    "Freedom": "Freedom",
    "Trust (Government Corruption)": "Corruption",
    "Generosity": "Generosity"
},inplace=True)
happiness16.drop(['Region', 'Lower Confidence Interval', 'Upper Confidence Interval', 'Dystopia Residual'], axis=1, inplace=True)

# 2017
happiness17.rename(columns={
    "Country": "Country",
    "Happiness.Rank": "Rank",
    "Happiness.Score": "Score",
    "Economy..GDP.per.Capita.": "GDP per Capita",
    "Health..Life.Expectancy.": "Life Expectancy",
    "Family": "Family/Social Support",
    "Freedom": "Freedom",
    "Trust..Government.Corruption.": "Corruption",
    "Generosity": "Generosity"
},inplace=True)
happiness17.drop(['Whisker.high', 'Whisker.low', 'Dystopia.Residual'], axis=1, inplace=True)

# 2018
happiness18.rename(columns={
    "Country or region": "Country",
    "Overall rank": "Rank",
    "Score": "Score",
    "GDP per capita": "GDP per Capita",
    "Healthy life expectancy": "Life Expectancy",
    "Social support": "Family/Social Support",
    "Freedom to make life choices": "Freedom",
    "Perceptions of corruption": "Corruption",
    "Generosity": "Generosity"
},inplace=True)

# 2019
happiness19.rename(columns={
    "Country or region": "Country",
    "Overall rank": "Rank",
    "Score": "Score",
    "GDP per capita": "GDP per Capita",
    "Healthy life expectancy": "Life Expectancy",
    "Social support": "Family/Social Support",
    "Freedom to make life choices": "Freedom",
    "Perceptions of corruption": "Corruption",
    "Generosity": "Generosity"
},inplace=True)

# Add year column to each data frame
# So after data is merged, its year information is available

happiness15["Year"] = 2015
happiness16["Year"] = 2016
happiness17["Year"] = 2017
happiness18["Year"] = 2018
happiness19["Year"] = 2019

# Merging data
# Creating one dataframe table for all of the years combined

frames = [happiness15, happiness16, happiness17, happiness18, happiness19]
happiness = pd.concat(frames)
happiness

# Once we have all the data in a single set,
# Clean the data

# Confirm there are no duplicates in the data set
happiness.duplicated().any()

# Check missing values in the data.
happiness.isna().sum()

# Check the missing value of corruption
happiness[happiness.Corruption.isna()]

# Mean imputation
# Replace NaN with mean corruption for United Arab Emirates
mean_corrup = happiness[happiness.Country == 'United Arab Emirates'].Corruption.mean()

happiness["Corruption"].fillna(mean_corrup, inplace = True)
happiness[(happiness.Country == 'United Arab Emirates') & (happiness.Year == 2018)]

# Double check no null values anymore
happiness.isna().sum()

"""## **Central Tendencies of happiness score over the years?  (Task 2)**
### Did they increase or decrease?
### By Iman Ali
"""

# Generate descriptive statistics of happiness score overall
happiness[['Score']].describe()

# Generate descriptive statistics of happiness score each year
happinessYearly = happiness.groupby('Year')
happinessYearly['Score'].describe()

# Yearly mean
happinessYearly = happinessYearly['Score'].mean()
happinessYearly

# Plotting the trend of happiness score over the years 
plt.figure(figsize=(10,6))
graph = sns.barplot(x="Year", y="Score", data=happiness)
plt.title("Mean Happiness Score per Year", fontsize=25);
plt.xlabel("Year", fontsize=15)
plt.ylabel("Happiness Score Mean", fontsize=15)


# Exaggerate the Y axis to show the slight difference in mean values
graph.set_ylim(5.3, 5.43)

# Plot the mean of Score of all countries or regions from 2015 to 2018
plt.figure(figsize=(10, 6))
plt.plot(happinessYearly, label="Mean Happiness score")

plt.title("Mean Happiness Score over the years", fontsize=25)
plt.xlabel("Year", fontsize=15)
plt.ylabel("Happiness Score Mean", fontsize=15)
plt.xticks([2015, 2016, 2017, 2018, 2019])
plt.show()

# Mean seems to increase each year by a very small amount
# The only exception is from 2016 to 2017 when the mean happiness
# score drops a bit, but then continues increasing again.

# Yearly median
happinessMedian = happiness.groupby('Year')['Score'].median()
happinessMedian

# Plot the median of Score of all countries or regions from 2015 to 2018
plt.figure(figsize=(10, 6))
graph = sns.barplot(["2015", "2016", "2017", "2018", "2019"], happinessMedian)

# Exaggerate the Y axis to show the slight difference in median values
graph.set_ylim(5.10, 5.43)

plt.title("Median Happiness Score over the years", fontsize=25)
plt.xlabel("Year", fontsize=15)
plt.ylabel("Happiness Score Median", fontsize=15)
plt.show()

# Plot the median of Score of all countries or regions from 2015 to 2018
plt.figure(figsize=(10, 6))
plt.plot(happinessMedian, label="Median Happiness score")

plt.title("Median Happiness Score over the years", fontsize=25)
plt.xlabel("Year", fontsize=15)
plt.ylabel("Happiness Score Median", fontsize=15)
plt.xticks([2015, 2016, 2017, 2018, 2019])
plt.show()

# Median follows the same trend as mean
# The median happiness score increases from 2015-2019
# with the exception of the year 2016 - 2017

"""## Analyzing Rankings (Task 3)
### by Rahul Raja
"""

# Which countries have stable rankings over the years? 

## we are going to analyze the overall change in rankings over the years
happiness_copy = happiness.copy()

grouped_happiness = happiness_copy.groupby("Country")["Country"].count()
grouped_happiness = grouped_happiness[grouped_happiness == 5]

hap15 = happiness15[['Country', 'Rank']]
hap16 = happiness16[['Country', 'Rank']]
hap17 = happiness17[['Country', 'Rank']]
hap18 = happiness18[['Country', 'Rank']]
hap19 = happiness19[['Country', 'Rank']]


countries = grouped_happiness.keys()
## print(countries)
total_change = []

##  Used for testing #### 
# country = "Canada"
# print(hap15.loc[hap15['Country'] == country].iat[0,1])
# print(hap16.loc[hap16['Country'] == country].iat[0,1])
# print(hap17.loc[hap17['Country'] == country].iat[0,1])
# print(hap18.loc[hap18['Country'] == country].iat[0,1])
# print(hap19.loc[hap19['Country'] == country].iat[0,1])


## iterating through all the countries to calculate the overall rank change from the year range [2015, 2019]
for i in range(len(countries)):
    country = countries[i]
    diff1 = abs(hap15.loc[hap15['Country'] == country].iat[0,1]- hap16.loc[hap16['Country'] == country].iat[0,1])
    ## print(diff1)
    diff2 = abs(hap16.loc[hap16['Country'] == country].iat[0,1]- hap17.loc[hap17['Country'] == country].iat[0,1])
    diff3 = abs(hap17.loc[hap17['Country'] == country].iat[0,1]- hap18.loc[hap18['Country'] == country].iat[0,1])
    diff4 = abs(hap18.loc[hap18['Country'] == country].iat[0,1]- hap19.loc[hap19['Country'] == country].iat[0,1])
    finalDiff = diff1 + diff2 + diff3 + diff4
    ## print(finalDiff)
    total_change.append(finalDiff)

## creating a new dataframe to store this information
data={"Countries":countries, "Rank_Change":total_change}
stable_table = pd.DataFrame(data)

## sort it by ascending values (low -> high)
stable_table = stable_table.sort_values(by=["Rank_Change"], ascending=True)
    
stable_table.head(20)  
    
## Rank Change is defined as the total amount of variation in the ranks between 2015 - 2019, which is a good measure for stable ranking.
## The top 20 countries with the most stable rankings are shown below.
## The top 5 include New Zealand, Iceland, Netherlands, Australia, and Canada.

## Which countries improved their rankings? 

## The way we are going to analyze this is by looking at overall chage from inital rank in 2015 to the last ranking that take place in 2019

## combining both 2015 and 2019 data together
frame2 = [happiness15, happiness19]
diff = pd.concat(frame2)

grouped_happiness = diff.groupby("Country")["Country"].count()
grouped_happiness = grouped_happiness[grouped_happiness == 2] ## use the info that is included in both of the years

countries = grouped_happiness.keys()

hap15 = happiness15[['Country', 'Rank']]
hap19 = happiness19[['Country', 'Rank']]
rank_change = []

#### Testing ### 
## country = "Afghanistan"
## print(hap15.loc[hap15['Country'] == country].iat[0,1])
## print(hap19.loc[hap19['Country'] == country].iat[0,1])

## iterating through to calculate the differences with the countries listed
for i in range(len(countries)):
  country = countries[i]
  rc = hap15.loc[hap15['Country'] == country].iat[0,1] - hap19.loc[hap19['Country'] == country].iat[0,1] ## if pos, then inc, else then dec
  rank_change.append(rc)

## print(rank_change)

## creating a dataframe to store the results   
data={"Countries":countries, "Overall_Change":rank_change}
overall_change_table = pd.DataFrame(data)


overall_change_table = overall_change_table.sort_values(by=["Overall_Change"], ascending=False) ## Since pos values are ones that increased rankings
    
overall_change_table.head(20)

## Table below shows the top 20 countries that have increased their happiness rankings from 2015 - 2019
## Top 5 countries include Benin, Ivory Coast, Honduras, Hungary, and Gabon

"""## Visualize the relationship between happiness score and other features such as GDP, social support, freedom, etc. (Task 4)
### by Rahul Raja
"""

# Testing the relationship between gdp and happiness scores

h_score = happiness["Score"]
gdp = happiness["GDP per Capita"]

plt.figure(figsize=(10, 8))
plt.plot(gdp,h_score , 'o', color='black'); 
plt.title("GDP per Capita vs Happiness Score",fontweight='bold', fontsize=25)
plt.xlabel("GDP per Capita", fontsize=15)
plt.ylabel("Happiness Score", fontsize=15)
plt.show()

# Correlation between gdp and happiness is 0.789

corr1, _ = pearsonr(gdp, h_score)
print('Pearsons correlation: %.3f' % corr1)

# Testing the relationship between social_support and happiness scores

social_support = happiness["Family/Social Support"]

plt.figure(figsize=(10, 8))
plt.plot(social_support ,h_score , 'o', color='red'); 
plt.title("Social Support vs Happiness Score",fontweight='bold', fontsize=25)
plt.xlabel("Social Support", fontsize=15)
plt.ylabel("Happiness Score", fontsize=15)
plt.show()

# Correlation between social support and happiness is 0.649, which is not as strong

corr2, _ = pearsonr(social_support, h_score)
print('Pearsons correlation: %.3f' % corr2)

## Testing the relationship between freedom and happiness scores

freedom = happiness["Freedom"]

plt.figure(figsize=(10, 8))
plt.plot(freedom ,h_score , 'o', color='blue'); 
plt.title("Freedom vs Happiness Score",fontweight='bold', fontsize=25)
plt.xlabel("Freedom", fontsize=15)
plt.ylabel("Happiness Score", fontsize=15)
plt.show()

# Correlation between freedom and happiness is 0.551, which is not as strong as the other two

corr3, _ = pearsonr(freedom, h_score)
print('Pearsons correlation: %.3f' % corr3)

## Testing the relationship between generosity and happiness scores
corruption = happiness["Generosity"]

plt.figure(figsize=(10, 8))
plt.plot(corruption ,h_score , 'o', color='green'); 
plt.title("Generosity vs Happiness Score",fontweight='bold', fontsize=25)
plt.xlabel("Generosity", fontsize=15)
plt.ylabel("Happiness Score", fontsize=15)
plt.show()

# Correlation between generosity and happiness is 0.138, which is not correlated at all

corr4, _ = pearsonr(corruption, h_score)
print('Pearsons correlation: %.3f' % corr4)

"""## Features contributing to happiness (Task 5)
### by Acero
"""

# What Features correlate to happiness? 
# Dictionary for graph
featureDict = {}
#What is the correlation between happiness and GDP per capita? Use Spearman because data is an interval type
hapScore = happiness["Score"]
gdp = happiness["GDP per Capita"]
gdpCorr, _ = pearsonr(gdp, hapScore)
featureDict["GDp per Capita"] = gdpCorr
print('GDP per Capita Pearsons correlation: %.3f' % gdpCorr)

# Correlation between family and happiness 
fam = happiness["Family/Social Support"]
famCorr, _ = pearsonr(fam, hapScore)
featureDict["Family"] = famCorr
print('Family value Pearsons correlation: %.3f' % famCorr)

# Correlation between life expectancy and happiness
lifeExp_val = happiness["Life Expectancy"]
LifeCorr, _ = pearsonr(lifeExp_val, hapScore)
featureDict["Life Expectancy"] = LifeCorr
print('Life Expectancy value Pearsons correlation: %.3f' % LifeCorr)

# Correlation between Freedom	and happiness
freedom_val = happiness["Freedom"]
freedomCorr = pearsonr(freedom_val, hapScore)
featureDict["Freedom"] = freedomCorr[0]
print('Freedom value Pearsons correlation: %.3f' % freedomCorr[0])

# Correlation between Corruption and happiness
corruption_val = happiness["Corruption"]
corruptionCorr = pearsonr(corruption_val, hapScore)
featureDict["Trust"] = corruptionCorr[0]
print('Trust in government value Pearsons correlation: %.3f' % corruptionCorr[0])

#Correlation between Generosity	and happiness 
generosity_val = happiness["Generosity"]
generosityCorr = pearsonr(generosity_val, hapScore)
featureDict["Generosity"] = generosityCorr[0]
print('Generosity value Pearsons correlation: %.3f' % generosityCorr[0])

#Year and happiness?
year_val = happiness["Year"]
yearCorr = pearsonr(year_val, hapScore)
featureDict["Year"] = yearCorr[0]
print('Year value Pearsons correelation: %.3f' % yearCorr[0])

print(featureDict)

factors = list(featureDict.keys())
corrVals = list(featureDict.values())
fig = plt.figure(figsize = (10, 8))
plt.bar(factors, corrVals, color ='blue', width = 0.4)

plt.xlabel("Factors", fontsize = 15)
plt.ylabel("Pearson Value Correlation", fontsize=15)
plt.title("Factors vs Pearson Value Correlation", fontsize=25)
plt.show()



"""# **If you are the president of a country, what would you do to make citizens happier?**

Judging by the factors that are most correlated with the happiness values, we can see that GDP per Capita, as well as Life expectancy are the most correlated. That means increasing GDP per Capita, as well as life expectancy may increase happiness in citizens.

To increase GDP per Capita:
Because GDP per Capita is the GDP divided by the population, one way to increase the value is by increasing the GDP by creating an economy that produces more products. By investing in industries, especially those that create many exports, the GDP can be increased. Increasing consumption in the country will also raise the GDP per Capita, and this can be done by adopting consumption stimulating policies, such as giving out stimulus checks.

To increase life Expectancy:
Invest in better health infrastructure, as well as health care. 
"""



"""# **Modeling and Question Answering**
predict the happiness rankings using machine learning approach

### Splitting into training and testing data (by Iman Ali)
"""

# Split into training and testing sets

# Training set => 2015 - 2018
# Testing set => 2019

# Extract training set
training = happiness[happiness.Year < 2019]

# Target variable 'Rank'. But we will predict the 'Score' and then calculate rank by 
# ordering scores in ascending order. Therefore use 'Score' as y
# Therefore Y_train has values of 'Score', and X_train with all other useful fields
Y_train = training[['Score']]
X_train = training[["GDP per Capita", "Life Expectancy", 'Family/Social Support', 'Freedom']]


# Display X_train
X_train

# Display Y_train (answers)
Y_train

# Extract testing set (2019)
testing = happiness[happiness.Year == 2019]

# Similar to training data,
# Y_test with values of 'Score', and X_test with all other fields
Y_test = testing[['Score']]
X_test = testing[["GDP per Capita", "Life Expectancy", 'Family/Social Support', 'Freedom']]


# Display X_test
X_test

# Display Y_test (answers)
Y_test

"""## **Multiple Linear Regression**
### Model 1 (by Iman Ali)
"""

################################# Set up a multiple linear regression model to train ################################

# Use linear regression from the sklearn library 
lin_reg = LinearRegression()
lin_reg.fit(X_train, Y_train)

# Use the coefficient and intercept to determine the y = mx + c linear fit
print("Coefficients: ", lin_reg.coef_)
print("Intercept: ", lin_reg.intercept_)

############################ Predict happines rank for each country in 2019 ############################

# Use the model to predict values of happiness score for the testing set
score_pred = lin_reg.predict(X_test)
score_pred = score_pred.flatten()
score_pred

# Comparing Score Prediction

# Actual values vs Predicted
x = pd.DataFrame({'Country': testing["Country"].values, 'Actual Score': Y_test['Score'].tolist(), 'Predicted Score': score_pred})
x

fig, ax = plt.subplots(figsize=(10, 8))
plt.scatter(Y_test, score_pred ,s = 10)
plt.xlabel('Actual Score',fontsize=15)
plt.ylabel('Predicted Score',fontsize=15)
plt.title('Actual vs Predicted Happiness Score 2019',fontweight='bold',fontsize=25)

min = np.min([ax.get_xlim(), ax.get_ylim()])
max = np.max([ax.get_xlim(), ax.get_ylim()])

ax.plot([min, max], [min, max])
ax.axis('equal')

# Comparing Actual vs Calculated Ranks
# (calculating ranks from predicted score values)

y = pd.DataFrame({'Country': testing["Country"].values, 'Actual Rank': testing["Rank"].values })
y['Predicted Rank'] = x['Predicted Score'].rank(ascending=False)
y['Predicted Rank'] = y['Predicted Rank'].astype(int)
y

#################################### How well/badly does the model work ##########################################

# Evaluate the correctness of your predictions based on the original â€œScoreâ€ column
print('Score:', lin_reg.score(X_test, Y_test))

"""Prediction score is around 74% which is quite decent"""

########################################### Root mean squared error #############################################

MSE = mean_squared_error(Y_test, score_pred, squared=False)
RMSE = np.sqrt(MSE) 
print('Mean Squared Error:', MSE)
print('Root Mean Squared Error:', RMSE)

# According to the sklearn library:
# Mean squared error return: the best value is 0.0 

# Lower values of MSE indicate better fit.
# Since we have quite a low value of 0.56 (close to zero) that means our model did a good job at predicting
# happiness scores and therefore accurate ranks

"""## **K Nearest Neighbor Regression**
### Model 2 (by Rahul Raja)
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV

parameters = {"n_neighbors": range(1, 50)}
gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)
gridsearch.fit(X_train, Y_train)

gridsearch.best_params_

## Reports best k value is 6

knn_model = KNeighborsRegressor(n_neighbors=6) ## can change this paramter

knn_model.fit(X_train, Y_train)

test_preds = knn_model.predict(X_test)
score_pred = test_preds.flatten()
score_pred

x = pd.DataFrame({'Country': testing["Country"].values, 'Actual Score': Y_test['Score'].tolist(), 'Predicted Score': score_pred})
x

fig, ax = plt.subplots(figsize=(10, 8))
plt.scatter(Y_test, score_pred ,s = 10)
plt.xlabel('Actual Score',fontsize=15)
plt.ylabel('Predicted Score',fontsize=15)
plt.title('Actual vs Predicted Happiness Score 2019 (KNN)',fontweight='bold',fontsize=25)

min = np.min([ax.get_xlim(), ax.get_ylim()])
max = np.max([ax.get_xlim(), ax.get_ylim()])

ax.plot([min, max], [min, max])
ax.axis('equal')

Actual = pd.DataFrame({'Country': testing["Country"].values, 'Actual': testing["Rank"].values })
Actual['Predicted'] = x['Predicted Score'].rank(ascending=False)
Actual['Predicted'] = Actual['Predicted'].astype(int)
Actual

print('Score:', knn_model.score(X_test, Y_test)) ## Accuracy of around 77.30%

test_mse = mean_squared_error(Y_test, score_pred, squared=False)
test_rmse = np.sqrt(test_mse)
print('Root Mean Squared Error:', test_rmse)

"""# **Random Forest Regression**

---




Model 3 (by Acero)
"""

# imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestRegressor


# make instance of Regressor

training2 = happiness[happiness.Year < 2019]

Y_training2 = training['Score']
X_training2 = training2[["GDP per Capita", "Life Expectancy", 'Family/Social Support', 'Freedom']]




regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)
regressor.fit(X_training2, Y_training2)
prediction = regressor.predict(X_test)
prediction = prediction.flatten()
prediction

# Comparing values
compareFrame = pd.DataFrame({'Country': testing["Country"].values, 'Actual Score': Y_test['Score'].tolist(), 'Predicted Score': prediction})
compareFrame

# Figure with scatter plot
fig, ax = plt.subplots(figsize=(10, 8))
plt.scatter(Y_test, prediction, s = 10)
plt.xlabel('Actual Score',fontsize=15)
plt.ylabel('Predicted Score',fontsize=15)
plt.title('Actual vs Predicted Happiness Score 2019 (Forest)',fontweight='bold',fontsize=25)

min = np.min([ax.get_xlim(), ax.get_ylim()])
max = np.max([ax.get_xlim(), ax.get_ylim()])

ax.plot([min, max], [min, max])
ax.axis('equal')

Actual = pd.DataFrame({'Country': testing["Country"].values, 'Actual': testing["Rank"].values })
Actual['Predicted'] = compareFrame['Predicted Score'].rank(ascending=False)
Actual['Predicted'] = Actual['Predicted'].astype(int)
Actual

print('Score:', regressor.score(X_test, Y_test))

"""**Accuracy of 61.67%, the worst of the three models**"""

test_mse = mean_squared_error(Y_test, prediction, squared=False)
test_rmse = np.sqrt(test_mse)
print('Root Mean Squared Error:', test_rmse)